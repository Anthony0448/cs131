TASK 1

1a and 1b) 
Used vim to edit bashrc. Checked the home directory with ls -a.
vim ~/.bashrc

Edited bashrc to contain:
alias l='ls -ltr'

total 4
-rw-rw-r-- 1 anthony_flores anthony_flores 163 Feb 12 23:26 a1.txt

alias w='ls -l | wc
      2      11      75

TASK 2

2a) just used mkdir in a1. No poutput.

Thw two pairs of pickup and dropoff locations I selected are (151.0, 239.0) and (7.0, 129) 

2b)
I used the command:
grep "151.0,239.0" 2019-01-h1.csv | cut -d',' -f11 > ./FARE/151.0-239..0.txt and grep "7.0,219.0" 2019-01-h1.csv | cut -d',' -f11 > ./FARE/7.0-129.0.txt

1. The grep part is used to filter between the sequence of pickup and dropoff location. This works since they are right next to each other.
2. This grep output is piped into cut -d',' -f11 which sets the delimiter as the commas found in the file and sets to cut out only the 11th position which holds the fare value. This results in only returning the fare values for a specific location.
3. > send output to file at specified path.

2c)
The command used to find the mean is echo "($(paste -s -d+ /home/anthony_flores/cs131/a1/FARE/7.0-129.0.txt)) / $(wc -l < /home/anthony_flores/cs131/a1/FARE/7.0-129.0.txt)" | bc -l >> a1.txt

The mean for the locations 7.0-129.0: 44.85800000000000000000
The mean for the locations 151.0-239.0: 6.64491942703670546105

This command is written through the echo command to take advantage of the bc command which reads an input as an expression and returns and output for the equation.

1. The paste command with -s puts all the lines of the page on one line and -d+ is the delimiter between lines seperated by a + symbol so that the bc command can read it as a sum.
2. the wc -l command returns the number of lines within the read file so we can get the value to divide by for the mean
3. We then pipe this echo command into bc with -l (for decimal value) so that we can calculate the mean as (sum of all entries) / (number of entries) = mean.

2d)
The highest amount paid by customers picked up on January 10, 2019 was 36090.3

The command used was:
grep 2019-01-10 2019-01-h1.csv | cut -d',' -f11 | sort -nr | head -1 >> a1.txt

1. grep is used to find entries that only occured on the specific date January 10, 2019
2. cut -d',' f-11 is used to only return the 11th column seperated by the ',' which is the column that represents amount paid.
3. sort -nr sorts the values numerally and the reverse makes it so the highest value appears at the top
4. head -1 returns the highest value only
5. print output to txt file

2e)
* The fourth column seperated by commas indicates the number of passengers

In order to search for rides that only include three or more customers I used this command:
grep -E '^([^,]*,){3}3\.0,' 2019-01-h1.csv

1. grep -E allows for an extended regex pattern to be used which lets us use different syntax
2. ^ starts at the beginning of the line, () encases a pattern to be repeated {3} times to reach the fourth column, [^,] skip over any character till it reaches a comma, * anything can be in the middle, ',' must end with a comma. This is done {3} times to reach the fourth column whiuch is contained between two commas.
3. The 3\.0 means to match a character 3, \ makes the period a normal character, then .0 matches the characters 3.0

The full command is:
grep -E '^([^,]*,){3}3\.0,' 2019-01-h1.csv | cut -d',' -f9 | sort -n | uniq -c | sort -nr | head -10 >> a1.txt

1. grep stuff is explained above.
2. The cut prints out only what is on column 9 which is the dropoff locations in the dataset
3. sort is needed in order to properly use uniq since uniq only counts sequential entries
4. uniq -c is used to count how many times a dropoff location appears to determine its popularity.
5. sort -nr is used to order the entries numerically and in reverse to show the highest at the top first
6. head -10 only displays the first 10 entries
7. print result to .txt file

   6391 236.0
   5700 230.0
   5635 161.0
   5151 237.0
   4670 48.0
   4428 142.0
   4382 170.0
   4290 239.0
   4284 162.0
   4004 186.0
