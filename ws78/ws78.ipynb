{"cells":[{"cell_type":"code","execution_count":8,"id":"8309c6f7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: gs://spark-data-bucket-a/ws8-sample-data/sugar_consumption_dataset.csv\n","+------------+----------------------------+\n","|     Country|avg(Total_Sugar_Consumption)|\n","+------------+----------------------------+\n","|       China|           5478268.558732333|\n","|      Mexico|          5722467.4505707435|\n","|     Germany|           5865260.984662195|\n","|       India|           5785490.384962714|\n","|      Brazil|            5648879.17567088|\n","|      Russia|           5776170.058888655|\n","|       Japan|           5866932.821403096|\n","|      France|          5574231.9196897885|\n","|         USA|           5233741.122788353|\n","|   Indonesia|            5619125.53808222|\n","|   Australia|           5780714.914088061|\n","|South Africa|           5413886.818597031|\n","+------------+----------------------------+\n","\n","Runtime: 1.9573 seconds\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","import time\n","\n","# List with the data paths\n","data = \"gs://spark-data-bucket-a/ws8-sample-data/sugar_consumption_dataset.csv\" # 3.6 MB\n","\n","# Create a SparkSession instance (the entry point for Spark)\n","spark = SparkSession.builder. appName (\"TASK2\") . getOrCreate ()\n","\n","print(f\"Reading: {data}\")\n","    \n","# Get the current time and set it as the start time \n","start_time = time.time()\n","    \n","# Read a file in CSV format into Spark DataFrame\n","df = spark.read.csv(data, header=True, inferSchema=True)\n","\n","# Groups the DataFrame using the specified columns and returns the average of the values in a group\n","df.groupBy('Country').avg('Total_Sugar_Consumption').show()\n","    \n","# The stopping time is set to the time at after a file is processed\n","end_time = time.time()\n","    \n","# The duration is the difference between the start and end time\n","# Format the runtime with 4 decimal placements\n","print(f\"Runtime: {(end_time - start_time):.4f} seconds\\n\")"]},{"cell_type":"code","execution_count":11,"id":"c7092fec","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: gs://spark-data-bucket-a/ws8-sample-data/major-crime-indicators.csv\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 18:=============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------+------------------+\n","|REPORT_YEAR|   avg(REPORT_DAY)|\n","+-----------+------------------+\n","|       2018|15.727501539614963|\n","|       2017|15.830392714854867|\n","|       2020|15.743406843242015|\n","|       2015|15.646727891983943|\n","|       2014|15.679553926249962|\n","|       2016| 15.82729929619468|\n","|       2019|15.741121052762754|\n","|       2024| 15.79137788516564|\n","|       2023|15.776318718005786|\n","|       2021| 15.78835376781498|\n","|       2022| 15.71933058800996|\n","+-----------+------------------+\n","\n","Runtime: 4.8341 seconds\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.sql import SparkSession\n","import time\n","\n","data = \"gs://spark-data-bucket-a/ws8-sample-data/major-crime-indicators.csv\" # 123 MB\n","\n","spark = SparkSession.builder. appName (\"TASK2\") . getOrCreate ()\n","\n","print(f\"Reading: {data}\")\n","    \n","start_time = time.time()\n","    \n","df = spark.read.csv(data, header=True, inferSchema=True)\n","\n","df.groupBy('REPORT_YEAR').avg('REPORT_DAY').show()\n","    \n","end_time = time.time()\n","    \n","print(f\"Runtime: {(end_time - start_time):.4f} seconds\\n\")"]},{"cell_type":"code","execution_count":12,"id":"645aec1c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: gs://spark-data-bucket-a/ws8-sample-data/Building_Permits.csv\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 23:=============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------+------------------+\n","|Permit Type|avg(Street Number)|\n","+-----------+------------------+\n","|          1| 932.2836676217765|\n","|          6|          1215.785|\n","|          3|1147.5320193684786|\n","|          4|1157.5971645919778|\n","|          8|1119.8784918700096|\n","|          5| 874.8021978021978|\n","|          2| 907.1252631578948|\n","|          7| 1287.849315068493|\n","+-----------+------------------+\n","\n","Runtime: 3.5966 seconds\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.sql import SparkSession\n","import time\n","\n","data = \"gs://spark-data-bucket-a/ws8-sample-data/Building_Permits.csv\" # 79 MB\n","\n","spark = SparkSession.builder. appName (\"TASK2\") . getOrCreate ()\n","\n","print(f\"Reading: {data}\")\n","    \n","start_time = time.time()\n","    \n","df = spark.read.csv(data, header=True, inferSchema=True)\n","\n","df.groupBy('Permit Type').avg('Street Number').show()\n","    \n","end_time = time.time()\n","    \n","print(f\"Runtime: {(end_time - start_time):.4f} seconds\\n\")"]},{"cell_type":"code","execution_count":15,"id":"c830d367","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: gs://spark-data-bucket-a/ws8-sample-data/Bitcoin Pulse Hourly Dataset from Markets Trends and Fear.csv\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 32:=============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+----------------+----------------------+\n","|BNB_USDT_1h_open|avg(ETH_USDT_1h_close)|\n","+----------------+----------------------+\n","|        308.6643|               1810.15|\n","|         239.215|               1907.69|\n","|          242.52|               1881.39|\n","|        230.6001|               1835.96|\n","|         244.042|               1888.09|\n","|        246.8231|                1863.0|\n","|        234.2985|                2364.0|\n","|        321.4898|                2276.8|\n","|        317.0691|               2513.12|\n","|        305.9497|               2303.54|\n","|        482.3066|               3900.55|\n","|        320.7826|               1897.96|\n","|        326.8171|               1900.77|\n","|         308.621|               1847.51|\n","|        238.2099|                1852.4|\n","|        240.9913|               1861.84|\n","|        211.6929|                1612.2|\n","|        235.8686|               2345.46|\n","|        360.3763|               2796.23|\n","|         603.784|               3326.27|\n","+----------------+----------------------+\n","only showing top 20 rows\n","\n","Runtime: 3.2350 seconds\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.sql import SparkSession\n","import time\n","\n","data = \"gs://spark-data-bucket-a/ws8-sample-data/Bitcoin Pulse Hourly Dataset from Markets Trends and Fear.csv\" # 20.3 MB\n","\n","spark = SparkSession.builder. appName (\"TASK2\") . getOrCreate ()\n","\n","print(f\"Reading: {data}\")\n","    \n","start_time = time.time()\n","    \n","df = spark.read.csv(data, header=True, inferSchema=True)\n","\n","df.groupBy('BNB_USDT_1h_open').avg('ETH_USDT_1h_close').show()\n","    \n","end_time = time.time()\n","    \n","print(f\"Runtime: {(end_time - start_time):.4f} seconds\\n\")"]},{"cell_type":"code","execution_count":24,"id":"c1963dc1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: gs://spark-data-bucket-a/ws8-sample-data/sstudent_engagement_dataset.csv\n","+----------+------------------+\n","|Student_ID|    avg(Delta_PSD)|\n","+----------+------------------+\n","|       148| 1.376976223786101|\n","|       243| 1.832131830295777|\n","|        65|1.4613077171072717|\n","|       133|1.6571829994841554|\n","|       126| 1.548071338309175|\n","|        81|1.4219201985448717|\n","|       300| 1.286084536706675|\n","|        76|1.5329057274947087|\n","|       192|1.5895542610453837|\n","|        12|1.6791203654876032|\n","|       209|1.6189119723053522|\n","|       122|1.4529684771981803|\n","|       140| 1.677434468108821|\n","|       177| 1.425804126017377|\n","|         1|1.4613562019151634|\n","|       206|1.4673143313219565|\n","|         6| 1.725644249995684|\n","|       205|1.3295001399820308|\n","|         3| 1.660069998194943|\n","|       142|1.2777072338293014|\n","+----------+------------------+\n","only showing top 20 rows\n","\n","Runtime: 1.0884 seconds\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","import time\n","\n","data = \"gs://spark-data-bucket-a/ws8-sample-data/sstudent_engagement_dataset.csv\" # 0.473 MB\n","\n","spark = SparkSession.builder. appName (\"TASK2\") . getOrCreate ()\n","\n","print(f\"Reading: {data}\")\n","    \n","start_time = time.time()\n","    \n","df = spark.read.csv(data, header=True, inferSchema=True)\n","\n","df.groupBy('Student_ID').avg('Delta_PSD').show()\n","    \n","end_time = time.time()\n","    \n","print(f\"Runtime: {(end_time - start_time):.4f} seconds\\n\")"]},{"cell_type":"code","execution_count":null,"id":"f0d918a2","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}