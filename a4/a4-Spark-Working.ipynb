{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701f6885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:46:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# This cell initiates a spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('a4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85279059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|PULocationID|DOLocationID|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            1.0|       151.0|       239.0|        9.95|\n",
      "|            1.0|       239.0|       246.0|        16.3|\n",
      "|            3.0|       236.0|       236.0|         5.8|\n",
      "|            5.0|       193.0|       193.0|        7.55|\n",
      "|            5.0|       193.0|       193.0|       55.55|\n",
      "|            5.0|       193.0|       193.0|       13.31|\n",
      "|            5.0|       193.0|       193.0|       55.55|\n",
      "|            1.0|       163.0|       229.0|        9.05|\n",
      "|            1.0|       229.0|         7.0|        18.5|\n",
      "|            2.0|       141.0|       234.0|        13.0|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'gs://dataproc-staging-northamerica-northeast2-450676183334-mwz98htd/notebooks/jupyter/2019-01-h1.csv'\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Select the columns to use manually\n",
    "trimmed_df = df.select(\"passenger_count\", \"PULocationID\", \"DOLocationID\", \"total_amount\")\n",
    "\n",
    "\n",
    "trimmed_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF, testDF = df.randomSplit([.8, .2])\n",
    "\n",
    "# Seed is used to make a split reproduceable? (unsure)\n",
    "# 0.8 and 0.2 split the dataset to 80% use for training and 20% for testing\n",
    "trainDF, testDF = trimmed_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bed681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3d2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vecAss = VectorAssembler(inputCols = ['passenger_count'], outputCol = \"features\")\n",
    "\n",
    "# Assemble to vector, features, \"passenger_count\", \"PULocationID\", \"DOLocationID\"\n",
    "vecAss = VectorAssembler(inputCols = [\"passenger_count\", \"PULocationID\", \"DOLocationID\"], outputCol=\"features\")\n",
    "\n",
    "# vecTrainDF = vecAss.transform(trainDF)\n",
    "\n",
    "# vecTrainDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87cd8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#lr = LinearRegression(featuresCol = 'features', labelCol = 'fare_amount')\n",
    "\n",
    "# Import the decision tree regressor instead of linear regression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Set max bins ; maxBins=512 (NOT NEEDED)\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c936b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58af473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Changed lt to dt for decision tree\n",
    "pipeline = Pipeline(stages = [vecAss, dt])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF) # ML transformer DF --> DF + prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9551d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------------+\n",
      "|passenger_count|PULocationID|DOLocationID|        prediction|\n",
      "+---------------+------------+------------+------------------+\n",
      "|            0.0|         4.0|         4.0| 17.69633136094663|\n",
      "|            0.0|         4.0|        33.0| 17.69633136094663|\n",
      "|            0.0|         4.0|        68.0|15.368079591565136|\n",
      "|            0.0|         4.0|        79.0|15.368079591565136|\n",
      "|            0.0|         4.0|       125.0|15.368079591565136|\n",
      "|            0.0|         4.0|       170.0|15.368079591565136|\n",
      "|            0.0|         7.0|         7.0| 17.69633136094663|\n",
      "|            0.0|         7.0|         7.0| 17.69633136094663|\n",
      "|            0.0|         7.0|       112.0|15.368079591565136|\n",
      "|            0.0|         7.0|       138.0|15.368079591565136|\n",
      "+---------------+------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# predDF = pipelineModel.transform(testDF)\n",
    "# predDF.show(4)\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"passenger_count\", \"PULocationID\", \"DOLocationID\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90dda09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.87634060519545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evalr = RegressionEvaluator(predictionCol='prediction',\n",
    "                            labelCol='total_amount',\n",
    "                            metricName='rmse')\n",
    "\n",
    "rmse = evalr.evaluate(predDF)\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
